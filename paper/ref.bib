@article{Sak2014,
    author = {Sak, Ha\c{c}sim and Senior, Andrew and Beaufays, Fran\c{c}oise},
    journal = {arXiv preprint arXiv:1402.1128},
    number = {Cd},
    title = {{Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition}},
    year = {2014}
}
@article{Gers2002,
    author = {Gers, Felix and Schraudolph, Nicol and Schmidhuber, J\"{u}rgen},
    journal = {Journal of Machine Learning Research},
    pages = {115--143},
    title = {{Learning Precise Timing with LSTM Recurrent Networks}},
    volume = {3},
    year = {2002}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
doi = {10.1162/neco.1997.9.8.1735},
issn = {0899-7667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@article{Hochreiter,
author = {Hochreiter, Sepp and Frasconi, Paolo},
title = {{Gradient Flow in Recurrent Nets : the Diffculty of Learning Long-Term Dependencies}}
}
@article{Monner2012,
abstract = {The long short term memory (LSTM) is a second-order recurrent neural network architecture that excels at storing sequential short-term memories and retrieving them many time-steps later. LSTM's original training algorithm provides the important properties of spatial and temporal locality, which are missing from other training approaches, at the cost of limiting its applicability to a small set of network architectures. Here we introduce the generalized long short-term memory(LSTM-g) training algorithm, which provides LSTM-like locality while being applicable without modification to a much wider range of second-order network architectures. With LSTM-g, all units have an identical set of operating instructions for both activation and learning, subject only to the configuration of their local environment in the network; this is in contrast to the original LSTM training algorithm, where each type of unit has its own activation and training instructions. When applied to LSTM architectures with peephole connections, LSTM-g takes advantage of an additional source of back-propagated error which can enable better performance than the original algorithm. Enabled by the broad architectural applicability of LSTM-g, we demonstrate that training recurrent networks engineered for specific tasks can produce better results than single-layer networks. We conclude that LSTM-g has the potential to both improve the performance and broaden the applicability of spatially and temporally local gradient-based training algorithms for recurrent neural networks.},
author = {Monner, Derek and Reggia, James A},
doi = {10.1016/j.neunet.2011.07.003},
issn = {1879-2782},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {Algorithms,Learning,Learning: physiology,Long-Term,Long-Term: physiology,Memory,Neural Networks (Computer),Short-Term,Short-Term: physiology,Time Factors},
month = jan,
number = {1},
pages = {70--83},
pmid = {21803542},
title = {{A generalized LSTM-like training algorithm for second-order recurrent neural networks.}},
url = {http://www.sciencedirect.com/science/article/pii/S0893608011002036},
volume = {25},
year = {2012}
}
@article{Sak2014a,
author = {Sak, Haşim and Senior, Andrew and Beaufays, Fran\c{c}oise},
month = feb,
title = {{Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition}},
url = {http://arxiv.org/abs/1402.1128},
year = {2014}
}
@article{Graves2005,
abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it. © 2005 Elsevier Ltd. All rights reserved.},
author = {Graves, Alex and Schmidhuber, J\"{u}rgen},
doi = {10.1109/IJCNN.2005.1556215},
isbn = {0780390482},
issn = {08936080},
journal = {Proceedings of the International Joint Conference on Neural Networks},
pages = {2047--2052},
pmid = {16112549},
title = {{Framewise phoneme classification with bidirectional LSTM networks}},
volume = {4},
year = {2005}
}
@article{Graves2004,
author = {Graves, Alex},
title = {{Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures}},
year = {2004}
}
@book{Graves2008,
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v1},
author = {Graves, Alex},
eprint = {arXiv:1308.0850v1},
isbn = {2000201075},
title = {{Supervised Sequence Labeling with Recurrent Neural Networks}},
year = {2008}
}
@article{Sutskever2008,
abstract = {The Temporal Restricted BoltzmannMachine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e., generate nice-looking samples of) several very high dimensional sequences, such as motion capture data and the pixels of low resolution videos of balls bouncing in a box. The major disadvantage of the TRBM is that exact inference is extremely hard, since even computing a Gibbs update for a single variable of the posterior is exponentially expensive. This difficulty has necessitated the use of a heuristic inference procedure, that nonetheless was accurate enough for successful learning. In this paper we introduce the Recurrent TRBM, which is a very slight modification of the TRBM for which exact inference is very easy and exact gradient learning is almost tractable. We demonstrate that the RTRBM is better than an analogous TRBM at generating motion capture and videos of bouncing balls.},
author = {Sutskever, Ilya and Hinton, Geoffrey and Taylor, Graham},
doi = {10.1.1.143.4232},
isbn = {9781605609492},
issn = {10477047},
journal = {Neural Information Processing Systems},
pages = {1601--1608},
title = {{The Recurrent Temporal Restricted Boltzmann Machine}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.143.4232\&amp;rep=rep1\&amp;type=pdf},
volume = {21},
year = {2008}
}
@article{Graves2013,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v5},
author = {Graves, Alex},
eprint = {arXiv:1308.0850v5},
journal = {arXiv preprint arXiv:1308.0850},
pages = {1--43},
title = {{Generating sequences with recurrent neural networks}},
url = {http://arxiv.org/abs/1308.0850},
year = {2013}
}
@article{Hreljac2000,
    author = {Hreljac, Alan and Marshall, Robert N.},
    journal = {Journal of Biomechanics},
    pages = {783--786},
    title = {{Algorithms to determine event timing during normal walking using kinematic data}},
    volume = {33},
    year = {2000}
}
@article{JurgenSchmidhuberDaanWierstra,
author = {{J\"{u}rgen Schmidhuber, Daan Wierstra}, Faustino Gomez},
title = {{Evolino: Hybrid neuroevolution / optimal linear search for sequence prediction}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.121.837}
}
@article{OConnor2007,
    author = {O'Connor, Ciara M. and Thorpe, Susannah K. and O'Malley, Mark J. and Vaughan, Christopher L.},
    journal = {Gait and Posture},
    pages = {469--474},
    title = {{Automatic detection of gait events using kinematic data}},
    volume = {25},
    year = {2007}
}
@article{Ghoussayni2004,
    journal = {Gait and Posture},
    pages = {266--272},
    title = {{Assessment and validation of a simple automated method for the detection of gait events and intervals}},
    volume = {20},
    year = {2004}
}
@article{Jasiewicz2006,
    author = {Jasiewicz, Jan M. and Allum, J. H J and Middleton, James W. and Barriskill, Andrew and Condie, Peter and Purcell, Brendan and Li, R. C T},
    journal = {Gait and Posture},
    pages = {502--509},
    title = {{Gait event detection using linear accelerometers or angular velocity transducers in able-bodied and spinal-cord injured individuals}},
    volume = {24},
    year = {2006}
}
@article{Miller2009,
    author = {Miller, Adam},
    journal = {Gait and Posture},
    pages = {542--545},
    title = {{Gait event detection using a multilayer neural network}},
    volume = {29},
    year = {2009}
}
@article{Hinton2012,
    author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
    journal = {arXiv: 1207.0580},
    pages = {1--18},
    title = {{Improving neural networks by preventing co-adaptation of feature detectors}},
    year = {2012}
}
@article{Pham2013,
    author = {Pham, Vu and Bluche, Th\'{e}odore and Kermorvant, Christopher and Louradour, J\'{e}r\^{o}me},
    journal = {arXiv preprint arXiv:1312.4569},
    title = {{Dropout improves Recurrent Neural Networks for Handwriting Recognition}},
    year = {2013}
}
@article{Dahl2013,
    author = {Dahl, George E. and Sainath, Tara N. and Hinton, Geoffrey E.},
    journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
    pages = {8609--8613},
    title = {{Improving deep neural networks for LVCSR using rectified linear units and dropout}},
    year = {2013}
}
@article{GravesSchmidhuber2005,
    author = {Graves, Alex and Schmidhuber, J\"{u}rgen},
    journal = {Neural Networks},
    pages = {602--610},
    title = {{Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures}},
    year = {2005}
}
@article{Liwicki2007,
    author = {Liwicki, Marcus and Graves, Alex and Bunke, Horst and Schmidhuber, J\"{u}rgen},
    journal = {Proceddings of the 9th International Conference on Document Analysis and Recognition, ICDAR 2007},
    title = {{A Novel Approach to On-Line Handwriting Recognition Based on Bidirectional Long Short-Term Memory Networks}},
    year = {2007}
}
@book{Graves2012,
	title = {Supervised Sequence Labelling with Recurrent Neural Networks},
	author = {Graves, Alex},
	year = {2012}
}
@book{Schuster1999,
    author = {Schuster, Michael},
    title = {{On Supervised Learning from Sequential Data With Applications for
    Speech Recognition}},
    year = {1999}
}
@book{Hochreiter1991,
    author = {Hochreiter, Sepp},
    title = {{Untersuchungen zu Dynamischen Neuronalen Netzen}},
    year = {1999}
}
